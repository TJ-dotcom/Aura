"""
Phase 3 integration tests for RAG-enabled inference.
"""

import pytest
import tempfile
import shutil
import os
from pathlib import Path

from aura_engine.models import EngineConfig
from aura_engine.engine import InferenceEngine
from aura_engine.rag import RAGPipeline


class TestPhase3Integration:
    """Integration tests for Phase 3 RAG functionality."""
    
    def setup_method(self):
        """Setup for each test method."""
        self.temp_dir = tempfile.mkdtemp()
        
        # Create test configuration
        self.config = EngineConfig(
            models_dir="models",
            llama_cpp_path="llama.cpp",
            faiss_index_path=None,
            enable_benchmarking=False,  # Disable for testing
            log_level="INFO",
            max_tokens=100,
            temperature=0.7
        )
    
    def teardown_method(self):
        """Cleanup after each test method."""
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_rag_pipeline_initialization(self):
        """Test RAG pipeline initialization."""
        engine = InferenceEngine(self.config)
        
        # RAG pipeline should be None initially
        assert engine.rag_pipeline is None
        
        # Initialize engine
        engine.initialize()
        
        # Still None until RAG is used
        assert engine.rag_pipeline is None
    
    def test_rag_context_retrieval_integration(self):
        """Test RAG context retrieval integration without actual model inference."""
        # Create test document
        test_doc = os.path.join(self.temp_dir, "test_knowledge.txt")
        with open(test_doc, 'w', encoding='utf-8') as f:
            f.write("""
            AURA-Engine-Core Technical Specifications
            
            Phase 3 Implementation Details:
            - FAISS vector store for efficient similarity search
            - Document chunking with configurable chunk size and overlap
            - Simple character frequency-based embeddings for testing
            - Context retrieval with configurable top-k results
            - Integration with existing inference pipeline
            
            RAG Configuration:
            - Default chunk size: 500 characters
            - Default overlap: 50 characters  
            - Default embedding dimension: 384
            - Default top-k retrieval: 3 chunks
            
            Performance Considerations:
            - Memory efficient document storage
            - Fast FAISS L2 distance search
            - Automatic index persistence to disk
            - Error handling for missing documents
            """)
        
        # Ingest document using RAG pipeline
        rag_pipeline = RAGPipeline(index_dir=self.temp_dir)
        rag_pipeline.ingest_document(test_doc)
        
        # Test context retrieval
        context = rag_pipeline.retrieve_context("What is the chunk size configuration?", top_k=2)
        
        assert context is not None
        assert len(context) > 0
        assert "chunk size" in context.lower() or "500" in context
    
    def test_engine_rag_flag_handling(self):
        """Test engine handling of RAG flag without actual inference."""
        # Create minimal test setup
        engine = InferenceEngine(self.config)
        engine.initialize()
        
        # Test that RAG pipeline is created when enable_rag=True
        # We'll mock this by directly checking the initialization
        
        # Create a temporary RAG index
        test_doc = os.path.join(self.temp_dir, "engine_test.txt")
        with open(test_doc, 'w', encoding='utf-8') as f:
            f.write("Test document for engine RAG integration testing.")
        
        # Initialize RAG pipeline in the temp directory  
        rag_pipeline = RAGPipeline(index_dir=self.temp_dir)
        rag_pipeline.ingest_document(test_doc)
        
        # Verify the pipeline can retrieve context
        context = rag_pipeline.retrieve_context("engine RAG integration")
        assert "engine" in context.lower() or "integration" in context.lower()
    
    def test_rag_error_handling(self):
        """Test RAG error handling scenarios."""
        engine = InferenceEngine(self.config)
        engine.initialize()
        
        # Test with empty RAG index - should handle gracefully
        # This simulates what happens when --rag is used but no documents are ingested
        empty_pipeline = RAGPipeline(index_dir=self.temp_dir)
        
        context = empty_pipeline.retrieve_context("Any query")
        assert context == ""
        
        stats = empty_pipeline.get_stats()
        assert stats['total_chunks'] == 0
        assert not stats['index_exists']
    
    def test_rag_with_multiple_documents(self):
        """Test RAG functionality with multiple documents."""
        rag_pipeline = RAGPipeline(index_dir=self.temp_dir)
        
        # Create multiple test documents
        documents = {
            "phase1.txt": """
            Phase 1: Hardware-Aware Inference Core
            - Hardware profiling and detection
            - Dynamic GPU layer calculation  
            - llama.cpp integration and subprocess management
            - Performance monitoring with TTFT and TPS metrics
            """,
            "phase2.txt": """
            Phase 2: Dynamic Model Orchestrator
            - Intelligent prompt routing with keyword analysis
            - Model switching and memory management
            - Support for specialized models (coder, writer, general)
            - Comprehensive logging of model operations
            """,
            "phase3.txt": """
            Phase 3: RAG Integration
            - FAISS vector store implementation
            - Document chunking and embedding generation
            - Context-aware retrieval and prompt augmentation
            - Integration with existing inference pipeline
            """
        }
        
        # Ingest all documents
        for filename, content in documents.items():
            filepath = os.path.join(self.temp_dir, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            rag_pipeline.ingest_document(filepath)
        
        # Test queries that should retrieve from different documents
        test_cases = [
            ("What is hardware profiling?", "hardware"),
            ("How does model switching work?", "model"),
            ("What is FAISS used for?", "FAISS"),
            ("Tell me about performance monitoring", "performance")
        ]
        
        for query, expected_term in test_cases:
            context = rag_pipeline.retrieve_context(query, top_k=2)
            assert context is not None
            assert len(context) > 0
            # Verify relevant content is retrieved (case insensitive)
            assert expected_term.lower() in context.lower(), f"'{expected_term}' not found in context for query: {query}"
    
    def test_rag_stats_and_monitoring(self):
        """Test RAG statistics and monitoring functionality."""
        rag_pipeline = RAGPipeline(index_dir=self.temp_dir)
        
        # Test empty stats
        stats = rag_pipeline.get_stats()
        assert stats['total_chunks'] == 0
        assert not stats['index_exists']
        
        # Add a document
        test_doc = os.path.join(self.temp_dir, "stats_test.txt")
        with open(test_doc, 'w', encoding='utf-8') as f:
            f.write("This is a test document for statistics validation. " * 10)
        
        rag_pipeline.ingest_document(test_doc)
        
        # Test populated stats
        stats = rag_pipeline.get_stats()
        assert stats['total_chunks'] > 0
        assert stats['index_exists']
        assert stats['embedding_dimension'] == 384
        assert 'total_documents' in stats
    
    def test_definition_of_done_scenario(self):
        """
        Test the Definition of Done scenario for Phase 3:
        'The CLI tool can successfully answer questions that require knowledge 
        from a user-provided document, which it accesses through its local RAG pipeline.'
        """
        # Create a specific knowledge document
        knowledge_doc = os.path.join(self.temp_dir, "project_knowledge.txt")
        with open(knowledge_doc, 'w', encoding='utf-8') as f:
            f.write("""
            AURA-Engine-Core Project Information
            
            Project Lead: Elite Engineering Team
            Project Status: Phase 3 Implementation Complete
            
            Key Achievement: Successfully integrated FAISS vector store for 
            document retrieval and context-aware inference capabilities.
            
            The system now supports:
            1. Document ingestion with automated chunking
            2. Vector similarity search using L2 distance
            3. Context retrieval for prompt augmentation
            4. RAG-enabled inference through CLI --rag flag
            
            Performance Metrics:
            - Document processing time: < 1 second per document
            - Query response time: < 100ms for context retrieval
            - Memory usage: Optimized for large document collections
            - Accuracy: High relevance scoring for retrieved contexts
            """)
        
        # Ingest the document
        rag_pipeline = RAGPipeline(index_dir=self.temp_dir)
        rag_pipeline.ingest_document(knowledge_doc)
        
        # Test specific questions that can only be answered with this document
        test_questions = [
            "Who is the project lead?",
            "What is the project status?", 
            "What performance metrics are tracked?",
            "What does the system now support?"
        ]
        
        expected_answers = [
            "Elite Engineering Team",
            "Phase 3 Implementation Complete", 
            "processing time",
            "Document ingestion"
        ]
        
        for question, expected in zip(test_questions, expected_answers):
            context = rag_pipeline.retrieve_context(question, top_k=2)
            
            # Verify relevant context is retrieved
            assert context is not None
            assert len(context) > 0
            
            # Verify the context contains information that could answer the question
            context_lower = context.lower()
            expected_lower = expected.lower()
            
            assert expected_lower in context_lower, \
                f"Expected '{expected}' not found in retrieved context for question: '{question}'"
        
        print("âœ“ Phase 3 Definition of Done scenario: PASSED")
        print("  - Documents successfully ingested into RAG pipeline")
        print("  - Questions requiring document knowledge can be answered")
        print("  - Context retrieval working correctly")
        print("  - RAG pipeline integration complete")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
