# BENCHMARKING STRATEGY

## 1. Mandate

Claims of performance are meaningless without data. This document defines the metrics we will use to prove the efficiency and viability of this system. These benchmarks are not for your information; they are the raw material for your resume bullet points and interview talking points.

## 2. Core Metrics

You will measure and record the following Key Performance Indicators (KPIs) for each test.

1.  **Time-to-First-Token (TTFT):** The latency (in milliseconds) from the moment the command is executed to the moment the first token is generated by the model. This is the primary measure of user-perceived responsiveness.
2.  **Tokens per Second (TPS):** The generation speed of the model after the prompt has been processed.
3.  **Peak VRAM Usage:** The maximum amount of GPU memory (in megabytes) consumed during the entire process, from model load to the end of inference.
4.  **Peak RAM Usage:** The maximum amount of system memory (in megabytes) consumed.
5.  **Model Load Time:** The time (in seconds) required to load the model into memory. For Phase 2, this includes the time to unload the previous model.

## 3. Benchmarking Scenarios

All tests will be conducted on the same hardware to ensure fair comparison.

* **Scenario A: Baseline Inference (Phase 1)**
* **Action:** Run a standard 200-word prompt on a single 7B model.
* **Goal:** Establish the baseline performance for TTFT, TPS, and memory footprint.

* **Scenario B: Model Switching Overhead (Phase 2)**
* **Action:** Run a coding prompt followed immediately by a writing prompt, forcing a model switch.
* **Goal:** Measure the Model Load Time for the switch. Record the peak RAM/VRAM and compare it to the baseline. The key metric is proving that peak memory usage does not equal the sum of both models.

* **Scenario C: RAG Pipeline Overhead (Phase 3)**
* **Action:** Run a RAG-enabled prompt on a small, indexed document base (~10 pages).
* **Goal:** Measure the increase in TTFT compared to the Baseline Inference. This quantifies the performance cost of the retrieval step.

## 4. Reporting

All benchmark results will be recorded in a separate `BENCHMARKS.md` file. Use a clear table format. For each entry, you must document the hardware used, the project phase, the scenario, and all measured metrics.

Example Entry:

| Date       | Hardware          | Phase | Scenario | TTFT (ms) | TPS    | Peak VRAM (MB) | Peak RAM (MB) |
|------------|-------------------|-------|----------|-----------|--------|----------------|---------------|
| 2025-08-19 | RTX 4080 / 32GB   | 1     | Baseline | 850       | 35.2   | 6250           | 8100          |

---

Data is your weapon. Collect it, analyze it, and use it to prove your value.